# How Embeddings are Trained: A Look at Word2Vec

Although it is common to use pre-trained embedding models, understanding how they are built is fundamental to a deep understanding of NLP. **Word2Vec** is one of the most influential and seminal approaches to training embeddings.

This document demystifies the Word2Vec training process, explaining its architecture and the two main models: Skip-gram and CBOW.

## 1. The Fake Task: Predicting Context to Learn Meaning

The genius of Word2Vec is not in creating a state-of-the-art classification model, but in using a classification task as a pretext (or "fake task") for a larger goal: learning vector representations of words.

The central idea is: **a word is characterized by the company it keeps**. If we train a neural network to predict neighboring words, it will be forced to learn high-quality representations for each word in order to perform this task well. It is these internal representations (the weights of the network) that will become our embeddings.

To create a dataset for this task, a **sliding window** is used that runs through the text, generating `(input_word, output_word)` pairs.

## 2. The Neural Network Architecture

Word2Vec uses a surprisingly simple and shallow neural network, usually with only one hidden layer.

The flow is as follows:

1.  **Input Layer:** The input word is provided to the network in **One-Hot Encoding** format. It is a sparse vector with the size of the vocabulary, containing `1` in the word's position and `0` in the others.

2.  **Hidden Layer:** This is the most important layer. It is a dense (fully connected) layer with no activation function. The number of neurons in this layer (`N`) is a hyperparameter that defines the **dimensionality of the embeddings** (e.g., 50, 100, 300).

3.  **Output Layer:** A layer with **Softmax** activation function. It produces a probability distribution over the entire vocabulary, indicating the chance of each word being the correct target word, given the input.

The model is then trained with the dataset generated by the sliding window to minimize the prediction error.

## 3. Extracting the Embeddings: The "Magic" of Word2Vec

After training the network, the output layer is discarded. What interests us is the weight matrix that connects the input layer to the hidden layer.

-   This weight matrix has dimensions `V x N`, where `V` is the size of the vocabulary and `N` is the dimension of the embedding.
-   When a word is passed as input (in One-Hot format), it effectively "selects" and activates the corresponding row in this weight matrix.
-   **This row of the weight matrix is the word's embedding.**

Thus, at the end of the process, this `V x N` matrix becomes our embedding lookup table.

## 4. The Two Word2Vec Models: Skip-gram vs. CBOW

Word2Vec is divided into two main architectures, which differ in how the prediction task is defined.

### Skip-gram

-   **Objective**: Predict context words (neighbors) from a single input word.
-   **Example**: Given `"dog"`, the model tries to predict `"the"` and `"barks"`.
-   **Advantages**: Generally performs better for rare words and in very large corpora.

![Skip-gram](https://i.imgur.com/3lT3b6V.png)

### CBOW (Continuous Bag-of-Words)

-   **Objective**: Predict a target word from its context (the neighboring words).
-   **Example**: Given `["the", "barks"]`, the model tries to predict `"dog"`.
-   **Technical Detail**: CBOW treats the context as a "bag of words" and usually averages the context's input vectors before passing them to the hidden layer.
-   **Advantages**: It is faster to train and tends to perform slightly better for frequent words.

![CBOW](https://i.imgur.com/9IAbm3G.png)

## 5. Training Your Own Model with Gensim

Although in practice we use pre-trained models, it is possible to train your own embeddings on a specific corpus with `gensim`.

```python
from gensim.models import Word2Vec
import gensim.downloader as api

# 1. Prepare the corpus (e.g., using a gensim test dataset)
corpus = api.load('text8')

# 2. Instantiate and train the Word2Vec model
# Main parameters:
# - sentences: The corpus, as a list of lists of tokens.
# - vector_size: The dimensionality of the embeddings (e.g., 150).
# - window: The maximum distance between the target word and its neighbors.
# - min_count: Ignores words with a total frequency lower than this value.
# - sg: Defines the training algorithm (1 for Skip-gram; 0 for CBOW).
# - workers: Number of CPU threads to use for training.

print("Training the Word2Vec model...")
model = Word2Vec(sentences=corpus,
                 vector_size=150,
                 window=5,
                 min_count=5,
                 sg=1, # Using Skip-gram
                 workers=4)
print("Training complete!")

# 3. Save the model for future use
model.save("word2vec_text8.model")

# 4. Load the saved model
# loaded_model = Word2Vec.load("word2vec_text8.model")

# 5. Explore the trained model
print("\nMost similar words to 'king':")
print(model.wv.most_similar('king'))
```

## Conclusion: Word2Vec and Beyond

Word2Vec laid the foundation for the modern era of word representations in NLP. Understanding its internal workings, the differences between Skip-gram and CBOW, and how to train a model from scratch are valuable skills.

After Word2Vec, other techniques emerged, each with its own innovations, such as **GloVe** (which optimizes a global co-occurrence matrix), **FastText** (which learns vectors for subwords, dealing better with rare and unknown words), and, more recently, contextual embeddings from **Transformer**-based models, such as **BERT**, which generate different representations for the same word depending on its context in the sentence.

