
# Text Generation with GPT-2 and Causal Language Modeling

**Causal Language Modeling (Causal LM)** is the task of predicting the next token in a sequence of text. Models like **GPT-2 (Generative Pre-trained Transformer)** are experts at this, using the Transformer's Decoder architecture to generate cohesive and contextually relevant text from an initial *prompt*.

In this guide, we will explore how to use GPT-2 for text generation, from using a pre-trained model to the process of **fine-tuning** to adapt the model to a specific knowledge domain, using the UTFPR regulations as our corpus.

## 1. What is Causal Language Modeling?

The goal of a causal language model is, given a sequence of words, to predict the next most likely word. By repeating this process, the model can generate sentences, paragraphs, and entire texts. It is the fundamental technology behind assistants like ChatGPT.

-   **Hugging Face Class:** `TFAutoModelForCausalLM` (for TensorFlow)

## 2. Text Generation with a Pre-trained Model

Pre-trained models already have a vast knowledge of the structure and semantics of a language. We can use them directly for generic text generation tasks.

### Step-by-Step

1.  **Load Tokenizer and Model:** We use the `transformers` library to download a pre-trained GPT-2 tokenizer and model for Portuguese.
2.  **Prepare the Input (Prompt):** We define an initial text for the model to complete.
3.  **Generate Text:** We use the `model.generate()` method to create the continuation of the text.
4.  **Decode the Output:** The tokenizer converts the IDs generated by the model back into readable text.

```python
from transformers import AutoTokenizer, TFAutoModelForCausalLM

# Load a small GPT-2 model for Portuguese
model_name = "pierreguillou/gpt2-small-portuguese"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = TFAutoModelForCausalLM.from_pretrained(model_name)

# Define the initial prompt
prompt = "The dog likes to walk on the street and"

# Tokenize the prompt and generate the text
input_ids = tokenizer(prompt, return_tensors="tf").input_ids
generated_text_ids = model.generate(input_ids, max_length=20)

# Decode and display the result
result = tokenizer.decode(generated_text_ids[0], skip_special_tokens=True)
print(result)
# Possible output: "The dog likes to walk on the street and his mother likes to play with him."
```

## 3. Fine-Tuning the Model

For the model to generate texts on a specific subject (such as academic regulations), we need to fine-tune it with a corpus from that domain. This process adjusts the weights of the pre-trained model, specializing it.

### Fine-Tuning Workflow

1.  **Prepare the Corpus:** We collect and clean the texts that will serve as the basis for training (in this case, the UTFPR regulations in HTML format).

2.  **Create a `Dataset`:** We load our corpus into a `datasets` library object and tokenize it. It is crucial that all sequences have the same length, using `padding` and `truncation`.

3.  **Prepare for Training:** `transformers` offers utilities like `DataCollatorForLanguageModeling` and `prepare_tf_dataset` that prepare the data for the Causal LM task, abstracting the complexity of creating input/output pairs (predicting the next word).

4.  **Compile and Train:** We compile the model with an optimizer (like `AdamWeightDecay`) and a loss function. Then, we start training with the `model.fit()` method.

```python
# (After loading and tokenizing the corpus into `tokenized_dataset`)

from transformers import DataCollatorForLanguageModeling

# Prepare the data for Causal LM training
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
tf_train_set = model.prepare_tf_dataset(
    tokenized_dataset,
    batch_size=16,
    shuffle=True,
    collate_fn=data_collator,
)

# Compile and start fine-tuning
model.compile(optimizer="adam", loss=model.hf_compute_loss)
model.fit(tf_train_set, epochs=5)
```

## 4. Results and Observations

After fine-tuning, the model becomes capable of generating more coherent texts within the specific domain.

-   **Prompt:** `"Validation is a procedure so that students can"`
-   **Result (After 100 epochs):** `"Validation is a procedure so that students can be enrolled in curricular units in higher education courses of another public higher education institution..."`

### Limitations and Challenges

-   **Hallucination:** Language models can generate information that seems plausible but is factually incorrect. In one of the tests with few epochs, when given the prompt about "coefficient," the model generated a text about "risk of death," an incorrect and meaningless association in the academic context. Increasing the number of training epochs helped to correct this behavior.
-   **Computational Cost:** Training Transformers is computationally intensive. Fine-tuning, even a small model on a modest corpus, can take hours on CPUs. The use of GPUs is highly recommended and, in many cases, indispensable.

Fine-tuning demonstrates the power of **Transfer Learning**: instead of training a model from scratch, we adapt an already powerful model to our specific needs, achieving great results with relatively less computational effort.
